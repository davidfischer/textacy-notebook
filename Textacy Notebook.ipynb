{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e21b806",
   "metadata": {},
   "source": [
    "## URLs\n",
    "\n",
    "These are real URLs that we want to classify or do keyword/topic extraction from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96464b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This URL points to Datascience/ML related content\n",
    "url1 = \"https://pytorch3d.readthedocs.io/en/latest/overview.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1870084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This URL points to Python and backend-web related content\n",
    "url2 = \"https://flask-mqtt.readthedocs.io/en/latest/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b121fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This URL points to Python, backend-web and DevOps related content\n",
    "url3 = \"https://huey.readthedocs.io/en/latest/guide.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ea687",
   "metadata": {},
   "source": [
    "## Topics & Keywords\n",
    "\n",
    "These are some topics and keywords we're interested in. Ideally, these are the topics and keywords to focus on but these topics could have additional keywords we haven't identified. There also might be different topics to \"discover\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2beab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = {\n",
    "    \"data-science\": {\n",
    "        \"name\": \"Data science & machine learning\",\n",
    "        \"keywords\": [\n",
    "            \"data-science\",\n",
    "            \"datascience\",\n",
    "            \"ml\",\n",
    "            \"training-data\",\n",
    "            \"model-training\",\n",
    "            \"machine-learning\",\n",
    "            \"machinelearning\",\n",
    "            \"sentiment-analysis\",\n",
    "            \"ai\",\n",
    "            \"artificial-intelligence\",\n",
    "            \"neural-net\",\n",
    "            \"neural-nets\",\n",
    "            \"data-analytics\",\n",
    "            \"visualization\",\n",
    "            \"data-visualization\",\n",
    "            \"nlp\",\n",
    "            \"object-detection\",\n",
    "            \"computer-vision\",\n",
    "            \"jupyter\",\n",
    "            \"matplotlib\",\n",
    "            \"deep-learning\",\n",
    "            \"pytorch\",\n",
    "            \"pydata\",\n",
    "            \"opencv-python-library\",\n",
    "            \"pandas\",\n",
    "            \"numpy\",\n",
    "            \"tensor\",\n",
    "            \"tensorflow\",\n",
    "        ],\n",
    "    },\n",
    "    \"backend-web\": {\n",
    "        \"name\": \"Backend web development\",\n",
    "        \"keywords\": [\n",
    "            \"backend\",\n",
    "            \"backend-web\",\n",
    "            \"flask\",\n",
    "            \"django\",\n",
    "            \"werkzeug\",\n",
    "            \"wsgi\",\n",
    "            \"celery\",\n",
    "            \"jinja\",\n",
    "        ],\n",
    "    },\n",
    "    \"frontend-web\": {\n",
    "        \"name\": \"Frontend web development\",\n",
    "        \"keywords\": [\n",
    "            \"frontend\",\n",
    "            \"frontend-web\",\n",
    "            \"javascript\",\n",
    "            \"react\",\n",
    "            \"reactjs\",\n",
    "            \"css\",\n",
    "            \"angular\",\n",
    "            \"angularjs\",\n",
    "            \"jquery\",\n",
    "            \"vuejs\",\n",
    "            \"vue\",\n",
    "            \"webpack\",\n",
    "            \"node\",\n",
    "            \"nodejs\",\n",
    "        ],\n",
    "    },\n",
    "    \"security-privacy\": {\n",
    "        \"name\": \"Security & privacy\",\n",
    "        \"keywords\": [\n",
    "            \"security\",\n",
    "            \"privacy\",\n",
    "            \"cryptography\",\n",
    "            \"oauth\",\n",
    "            \"authorization\",\n",
    "            \"authentication\",\n",
    "        ],\n",
    "    },\n",
    "    \"devops\": {\n",
    "        \"name\": \"DevOps\",\n",
    "        \"keywords\": [\n",
    "            \"devops\",\n",
    "            \"cloud\",\n",
    "            \"docker\",\n",
    "            \"kubernetes\",\n",
    "            \"container\",\n",
    "            \"containers\",\n",
    "            \"ansible\",\n",
    "            \"serverless\",\n",
    "            \"openshift\",\n",
    "            \"aws\",\n",
    "            \"linux\",\n",
    "            \"ubuntu\",\n",
    "            \"monitoring\",\n",
    "            \"openid-connect\",\n",
    "            \"oauth\",\n",
    "            \"redis\",\n",
    "            \"rabbitmq\",\n",
    "            \"nosql\",\n",
    "            \"postgres\",\n",
    "            \"postgresql\",\n",
    "            \"mysql\",\n",
    "            \"database\",\n",
    "            \"elasticsearch\",\n",
    "            \"lucene\",\n",
    "            \"solr\",\n",
    "            \"terraform\",\n",
    "            \"nginx\",\n",
    "        ],\n",
    "    },\n",
    "    \"python\": {\n",
    "        \"name\": \"Python development\",\n",
    "        \"keywords\": [\"python\", \"django\", \"flask\"],\n",
    "    },\n",
    "    \"game-dev\": {\n",
    "        \"name\": \"Game development\",\n",
    "        \"keywords\": [\"gamedev\", \"minecraft\", \"godot\", \"game\"],\n",
    "    },\n",
    "    \"blockchain\": {\n",
    "        \"name\": \"Blockchain\",\n",
    "        \"keywords\": [\n",
    "            \"blockchain\",\n",
    "            \"ethereum\",\n",
    "            \"bitcoin\",\n",
    "            \"cryptocurrency\",\n",
    "            \"hyperledger\",\n",
    "            \"solidity\",\n",
    "        ],\n",
    "    },\n",
    "    \"techwriting\": {\n",
    "        \"name\": \"Technical writing\",\n",
    "        \"keywords\": [\"technical-writing\", \"sphinx\", \"sphinx-doc\", \"mkdocs\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c3450",
   "metadata": {},
   "source": [
    "## Analyzers\n",
    "\n",
    "These are some different analyzers we are already using as well as some proposed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcd4610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a73f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAnalyzerBackend:\n",
    "\n",
    "    \"\"\"Base class that all analyzers should extend.\"\"\"\n",
    "\n",
    "    def __init__(self, url, **kwargs):\n",
    "        \"\"\"Base constructor.\"\"\"\n",
    "        self.url = url\n",
    "        self.user_agent = \"EthicalAds Analyzer\"\n",
    "\n",
    "    def fetch(self, **kwargs):\n",
    "        \"\"\"Performs a URL fetch on the analyzed URL.\"\"\"\n",
    "        # Unless specifically following redirects, don't bother\n",
    "        # Something is probably wrong if the request returns a redirect\n",
    "        kwargs.setdefault(\"allow_redirects\", False)\n",
    "        kwargs.setdefault(\"timeout\", 3)  # seconds\n",
    "        kwargs.setdefault(\"headers\", {\"user-agent\": self.user_agent})\n",
    "\n",
    "        try:\n",
    "            return requests.get(self.url, **kwargs)\n",
    "        except (requests.exceptions.RequestException, urllib3.exceptions.HTTPError):\n",
    "            pass\n",
    "            # log.info(\"Error analyzing URL: %s\", self.url, exc_info=True)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def analyze(self):\n",
    "        \"\"\"\n",
    "        Fetch the response and parse it for keywords.\n",
    "\n",
    "        :returns list: a list of keywords or `None` if the URL doesn't respond.\n",
    "        \"\"\"\n",
    "        resp = self.fetch()\n",
    "\n",
    "        if resp and resp.ok:\n",
    "            return self.analyze_response(resp)\n",
    "\n",
    "        # A failed request results in `None`.\n",
    "        return None\n",
    "\n",
    "    def analyze_response(self, resp):\n",
    "        \"\"\"\n",
    "        Analyze an HTTP response and return keywords/topics for the URL.\n",
    "\n",
    "        This will only be passed a successful response (20x).\n",
    "        All responses should return a list of keywords even if that list is empty.\n",
    "\n",
    "        This needs to be defined by subclasses\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses should define this.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c06d9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Naive keyword analyzer that is simply based on keyword counts.\"\"\"\n",
    "import collections\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class NaiveKeywordAnalyzerBackend(BaseAnalyzerBackend):\n",
    "\n",
    "    \"\"\"\n",
    "    A very naive keyword analyzer.\n",
    "\n",
    "    This mimics the results of our ad client.\n",
    "    \"\"\"\n",
    "\n",
    "    # CSS selectors to select the \"main\" content of the page\n",
    "    # The first of these to match anything is used\n",
    "    MAIN_CONTENT_SELECTORS = (\n",
    "        \"[role='main']\",\n",
    "        \"main\",\n",
    "        \"body\",\n",
    "    )\n",
    "\n",
    "    MAX_WORDS_ANALYZED = 1000\n",
    "    MAX_KEYWORDS = 3\n",
    "    MIN_KEYWORD_OCCURRENCES = 2\n",
    "\n",
    "    def __init__(self, url, **kwargs):\n",
    "        \"\"\"Overrides to get the keyword corpus.\"\"\"\n",
    "        super().__init__(url, **kwargs)\n",
    "\n",
    "        self.topics = TOPICS\n",
    "        self.keywords = set()\n",
    "        for t in self.topics:\n",
    "            for kw in self.topics[t][\"keywords\"]:\n",
    "                self.keywords.add(kw)\n",
    "\n",
    "    def analyze_response(self, resp):\n",
    "        \"\"\"Analyze an HTTP response and return a list of keywords/topics for the URL.\"\"\"\n",
    "        keywords = []\n",
    "\n",
    "        soup = BeautifulSoup(resp.content, features=\"html.parser\")\n",
    "\n",
    "        for selector in self.MAIN_CONTENT_SELECTORS:\n",
    "            results = soup.select(selector, limit=1)\n",
    "\n",
    "            # If no results, go to the next selector\n",
    "            # If results are found, use these and stop looking at the selectors\n",
    "            if results:\n",
    "                text = results[0].get_text().replace(\"\\n\", \" \")\n",
    "                keywords = self.analyze_text(text)\n",
    "                break\n",
    "\n",
    "        return keywords\n",
    "\n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"Analyze a large string of text for keyword extraction.\"\"\"\n",
    "        # Differs from string.punctuation in that the hyphen is missing\n",
    "        punctuation = r\"\"\"!\"#$%&'()*+,./:;<=>?@[\\]^_`{|}~\"\"\"\n",
    "\n",
    "        word_counter = collections.Counter()\n",
    "\n",
    "        for index, word in enumerate(text.split()):\n",
    "            if index > self.MAX_WORDS_ANALYZED:\n",
    "                break\n",
    "\n",
    "            # Remove punctuation and make it lowercase\n",
    "            word = word.translate(str.maketrans(\"\", \"\", punctuation)).lower()\n",
    "\n",
    "            if word in self.keywords:\n",
    "                word_counter[word] += 1\n",
    "\n",
    "        # Remove items with fewer than MIN_KEYWORD_OCCURRENCES\n",
    "        word_counter = collections.Counter(\n",
    "            {k: v for k, v in word_counter.items() if v >= self.MIN_KEYWORD_OCCURRENCES}\n",
    "        )\n",
    "\n",
    "        # Return the top MAX_KEYWORDS\n",
    "        return [kw for kw, _ in word_counter.most_common(self.MAX_KEYWORDS)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caa225bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['postgres', 'serverless', 'minecraft', 'reactjs', 'frontend-web']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(NaiveKeywordAnalyzerBackend(\"\").keywords)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e91e866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pytorch', 'backend']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze URL1 which should probably be ML/Datascience related\n",
    "analyzer = NaiveKeywordAnalyzerBackend(url1)\n",
    "analyzer.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e624dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flask', 'python']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze URL2 which should probably be Python/backend related\n",
    "analyzer = NaiveKeywordAnalyzerBackend(url2)\n",
    "analyzer.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c313c3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze URL3 which should probably be Python/backend/devops related\n",
    "analyzer = NaiveKeywordAnalyzerBackend(url3)\n",
    "analyzer.analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de63953",
   "metadata": {},
   "source": [
    "**NOTE:** This last URL wasn't classified correctly. No real identifying stuff was in the first few paragraphs so it was misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd309b4a",
   "metadata": {},
   "source": [
    "### A textacy-based analyzer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abe9a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "from textacy import preprocessing\n",
    "# from textacy.extract import keyterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e41ffb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextacyAnalyzerBackend(NaiveKeywordAnalyzerBackend):\n",
    "    def analyze_response(self, resp):\n",
    "        \"\"\"Analyze an HTTP response and return a list of keywords/topics for the URL.\"\"\"\n",
    "        keywords = []\n",
    "\n",
    "        soup = BeautifulSoup(resp.content, features=\"html.parser\")\n",
    "\n",
    "        for selector in self.MAIN_CONTENT_SELECTORS:\n",
    "            results = soup.select(selector, limit=1)\n",
    "\n",
    "            # If no results, go to the next selector\n",
    "            # If results are found, use these and stop looking at the selectors\n",
    "            if results:\n",
    "                text = results[0].get_text()\n",
    "                keywords = self.analyze_text(text)\n",
    "                break\n",
    "\n",
    "        return keywords\n",
    "    \n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"Analyze a large string of text for keyword extraction.\"\"\"\n",
    "        preproc = preprocessing.make_pipeline(\n",
    "            preprocessing.normalize.unicode,\n",
    "            preprocessing.remove.punctuation,\n",
    "            preprocessing.normalize.whitespace,\n",
    "        )\n",
    "        \n",
    "        # Followed tips from the Textacy official docs\n",
    "        # https://textacy.readthedocs.io/en/latest/quickstart.html\n",
    "        processed_text = preproc(text).lower()\n",
    "        en = textacy.load_spacy_lang(\"en_core_web_sm\", disable=(\"parser\",))\n",
    "        doc = textacy.make_spacy_doc(processed_text, lang=en)\n",
    "        keywords = []\n",
    "        \n",
    "        for phrase, _ in textacy.extract.keyterms.textrank(doc, normalize=\"lemma\", topn=10):\n",
    "            # Consider using the 2nd parameter - weight\n",
    "            keywords.append(phrase)\n",
    "        \n",
    "        # The ngrams extractor didn't seem to work quite as well\n",
    "        #keywords = list(textacy.extract.ngrams(doc, self.MAX_KEYWORDS, min_freq=self.MIN_KEYWORD_OCCURRENCES))\n",
    "        \n",
    "        return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a5d3d55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pytorch3d https github com facebookresearch synsin',\n",
       " 'pytorch3d codebase',\n",
       " 'pytorch3d renderer',\n",
       " 'mesh r cnn codebase',\n",
       " 'fair pytorch3d',\n",
       " 'pytorch3d useful',\n",
       " 'pytorch3d documentation',\n",
       " 'arxiv https arxiv org ab',\n",
       " 'facebook ai research computer vision team',\n",
       " 'differentiable mesh renderer']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze URL1 which should probably be ML/Datascience related\n",
    "analyzer = TextacyAnalyzerBackend(url1)\n",
    "analyzer.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb8c543c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flask mqtt',\n",
       " 'paho mqtt package',\n",
       " 'mosquitto mqtt server',\n",
       " 'mqtt s documentation',\n",
       " 'multiple iot device',\n",
       " 'mqtt client',\n",
       " 'mqtt integration',\n",
       " 'flask application',\n",
       " 'multiple worker',\n",
       " 'flask extension']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze URL2 which should probably be Python/backend related\n",
    "analyzer = TextacyAnalyzerBackend(url2)\n",
    "analyzer.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "403abd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['huey periodic task crontab minute= 0 hour=',\n",
       " 'huey periodic task schedule',\n",
       " 'task task',\n",
       " 'def print signal args signal task exc',\n",
       " 'huey task retries=2 retry',\n",
       " 'signal = = signal error',\n",
       " 'time sensitive task schedule',\n",
       " 'huey lock task report lock',\n",
       " 'huey lock task method',\n",
       " 'huey periodic task decorator']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze URL3 which should probably be Python/backend/devops related\n",
    "analyzer = TextacyAnalyzerBackend(url3)\n",
    "analyzer.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b5d786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
